{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARC | CompressARC + Easiest-First Strategy (FIXED)\n",
    "\n",
    "**Fixed version: submission.json \uc0dd\uc131 \ubcf4\uc7a5**\n",
    "\n",
    "References:\n",
    "- [\u6697\u9ed1AGI](https://www.kaggle.com/code/boristown/agi-compressarc)\n",
    "- [ARC-AGI Without Pretraining](https://www.kaggle.com/code/iliao2345/arc-agi-without-pretraining)\n",
    "\n",
    "In this notebook:\n",
    "* The simpler tasks, which are possible to solve, are prioritized to solve first and have the longer training time than difficult tasks\n",
    "* The number of training epochs is based on the simplicity score, and is different for each task\n",
    "* The simplicity score is determined by the amount of colors and pixels in the input\n",
    "\n",
    "Expected Score: **4.58** (Gold Medal \ud83e\udd47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "fake_mode = not os.getenv('KAGGLE_IS_COMPETITION_RERUN')\n",
    "\n",
    "def set_all_seeds(seed=GLOBAL_SEED):\n",
    "    \"\"\"\u8bbe\u7f6e\u6240\u6709\u53ef\u80fd\u7684\u968f\u673a\u79cd\u5b50\u6765\u786e\u4fdd\u53ef\u91cd\u73b0\u6027\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # \u4e3a\u4e86\u5b8c\u5168\u786e\u5b9a\u6027\uff0c\u7981\u7528CUDA\u7684\u975e\u786e\u5b9a\u6027\u7b97\u6cd5\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # \u8bbe\u7f6ePython\u54c8\u5e0c\u79cd\u5b50\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "#set_all_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import importlib\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append('/kaggle/input/publiccompressarc')\n",
    "\n",
    "# This little block of code does \"import preprocessing\" but avoids a name collision with another module\n",
    "module_path = \"/kaggle/input/publiccompressarc/preprocessing.py\"\n",
    "module_name = \"preprocessing\"\n",
    "spec = importlib.util.spec_from_file_location(module_name, module_path)\n",
    "preprocessing = importlib.util.module_from_spec(spec)\n",
    "sys.modules[module_name] = preprocessing\n",
    "spec.loader.exec_module(preprocessing)\n",
    "import train\n",
    "import arc_compressor\n",
    "import initializers\n",
    "import multitensor_systems\n",
    "import layers\n",
    "import solution_selection\n",
    "import visualization\n",
    "import solve_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting all the task names, setting defaults and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_default_device('cuda')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    start_time = time.time()\n",
    "    end_time = start_time + 12*3600 - 600\n",
    "\n",
    "    n_cpus = multiprocessing.cpu_count()\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "\n",
    "    # Find all the puzzle names\n",
    "    split = \"evaluation\" if fake_mode else \"test\"\n",
    "    with open(f'../input/arc-prize-2025/arc-agi_{split}_challenges.json', 'r') as f:\n",
    "        problems = json.load(f)\n",
    "    task_names = list(problems.keys())\n",
    "    n_tasks = len(task_names)\n",
    "\n",
    "    # Compute and sort the easy tasks to train\n",
    "    simplicty_scores = list()\n",
    "    for i in range(n_tasks):\n",
    "        input_matrix = np.array(list(problems.values())[i]['test'][0]['input'])\n",
    "        unique_values, counts = np.unique(input_matrix, return_counts=True)\n",
    "        \n",
    "        color_score = 1 - len(unique_values) / 11\n",
    "        pixel_score = 1 - (input_matrix.shape[0]*input_matrix.shape[1] / (31*31))\n",
    "        simplicty_score = color_score*10+pixel_score + 1\n",
    "        simplicty_scores.append(simplicty_score)\n",
    "    simplicty_scores = np.sqrt(np.array(simplicty_scores))\n",
    "    sorted_taskid = np.argsort(-simplicty_scores)\n",
    "    del problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that can spawn processes and schedule them on GPUs to take up each GPUs quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_runs(gpu_quotas, task_usages, n_iteration_list, verbose=False):\n",
    "    gpu_quotas = gpu_quotas[:]\n",
    "    # Schedule the tasks greedily to max out memory usage\n",
    "    t = time.time()\n",
    "    tasks_started = [False for i in range(n_tasks)]\n",
    "    tasks_finished = [False for i in range(n_tasks)]\n",
    "    processes = [None for i in range(n_tasks)]\n",
    "    process_gpu_ids = [None for i in range(n_tasks)]\n",
    "    with multiprocessing.Manager() as manager:\n",
    "        memory_dict = manager.dict()\n",
    "        solutions_dict = manager.dict()\n",
    "        error_queue = manager.Queue()\n",
    "        while not all(tasks_finished):\n",
    "            if not error_queue.empty():\n",
    "                raise ValueError(error_queue.get())\n",
    "            for i in sorted_taskid: #range(n_tasks): #\n",
    "                if tasks_started[i] and not tasks_finished[i]:\n",
    "                    processes[i].join(timeout=0)\n",
    "                    if not processes[i].is_alive():\n",
    "                        tasks_finished[i] = True\n",
    "                        gpu_quotas[process_gpu_ids[i]] += task_usages[i]\n",
    "                        if verbose:\n",
    "                            print(task_names[i], 'finished on gpu', process_gpu_ids[i],\n",
    "                                  'New quota is', gpu_quotas[process_gpu_ids[i]])\n",
    "            for gpu_id in range(n_gpus):\n",
    "                for i in sorted_taskid: #range(n_tasks): #\n",
    "                    enough_quota = gpu_quotas[gpu_id] > task_usages[i]\n",
    "                    enough_cpus = sum(map(int, tasks_started)) - sum(map(int, tasks_finished)) < n_cpus\n",
    "                    if not tasks_started[i] and enough_quota and enough_cpus:\n",
    "                        gpu_quotas[gpu_id] -= task_usages[i]\n",
    "                        args = (task_names[i], split, end_time, n_iteration_list[i], gpu_id, memory_dict, solutions_dict, error_queue)\n",
    "                        #args = (task_names[i], split, end_time, n_iteration_list, gpu_id, memory_dict, solutions_dict, error_queue)\n",
    "                        p = multiprocessing.Process(target=solve_task.solve_task, args=args)\n",
    "                        p.start()\n",
    "                        processes[i] = p\n",
    "                        tasks_started[i] = True\n",
    "                        process_gpu_ids[i] = gpu_id\n",
    "                        if verbose:\n",
    "                            print(task_names[i], 'started on gpu', process_gpu_ids[i],\n",
    "                                  'New quota is', gpu_quotas[process_gpu_ids[i]])\n",
    "            time.sleep(1)\n",
    "        if not error_queue.empty():\n",
    "            raise ValueError(error_queue.get())\n",
    "        memory_dict = dict(memory_dict)\n",
    "        solutions_dict = dict(solutions_dict)\n",
    "    time_taken = time.time() - t\n",
    "    if verbose:\n",
    "        print('All jobs finished in', time_taken, 'seconds.')\n",
    "    return memory_dict, solutions_dict, time_taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the amount of memory used for every task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    gpu_memory_quotas = [torch.cuda.mem_get_info(i)[0] for i in range(n_gpus)]\n",
    "\n",
    "    gpu_task_quotas = [int(gpu_memory_quota // (4 * 1024**3)) for gpu_memory_quota in gpu_memory_quotas]\n",
    "    task_usages = [1 for i in range(n_tasks)]\n",
    "    memory_dict, _, _ = parallelize_runs(gpu_task_quotas, task_usages, 2*np.ones(n_tasks, dtype=int), verbose=False)\n",
    "    #memory_dict, _, _ = parallelize_runs(gpu_task_quotas, task_usages, 2, verbose=False)\n",
    "    \n",
    "    # Sort the tasks by decreasing memory usage\n",
    "    tasks = sorted(memory_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    task_names, task_memory_usages = zip(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the time taken, while saturating memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #test_steps = 2000\n",
    "    test_steps = 500 if fake_mode else 2000\n",
    "    iterations_list = (1.0+simplicty_scores*test_steps/sum(simplicty_scores)).astype(int) #2000 is the total number of iterations for distributing\n",
    "    \n",
    "    safe_gpu_memory_quotas = [memory_quota - 6 * 1024**3 for memory_quota in gpu_memory_quotas]\n",
    "\n",
    "    #_, _, time_taken = parallelize_runs(safe_gpu_memory_quotas, task_memory_usages, test_steps, verbose=False)\n",
    "    _, _, time_taken = parallelize_runs(safe_gpu_memory_quotas, task_memory_usages, iterations_list, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the solution for every task, while saturating memory and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    time_per_step = time_taken / test_steps\n",
    "    time_left = end_time - time.time()\n",
    "    # n_steps = 5 if fake_mode else int(time_left // time_per_step)\n",
    "    # _, solutions_dict, time_taken = parallelize_runs(safe_gpu_memory_quotas, task_memory_usages, n_steps, verbose=True)\n",
    "    \n",
    "    #test_steps = int(2.0 * time_left // time_per_step)\n",
    "    test_steps = 500 if fake_mode else int(2.0 * time_left // time_per_step) #change iteration number here\n",
    "    iterations_list = (1.0+simplicty_scores*test_steps/sum(simplicty_scores)).astype(int) #2000 is the total number of iterations for distributing\\\n",
    "    _, solutions_dict, time_taken = parallelize_runs(safe_gpu_memory_quotas, task_memory_usages, iterations_list, verbose=True)\n",
    "    \n",
    "    # Format the solutions and put into submission file\n",
    "    with open('submission.json', 'w') as f:\n",
    "        json.dump(solutions_dict, f, indent=4)\n",
    "        \n",
    "    print(n_tasks, 'tasks solved.')\n",
    "    #print(n_steps, 'steps taken.')\n",
    "    print(time_taken, 'seconds taken.')\n",
    "    \n",
    "    # IMPORTANT: Ensure submission.json is preserved\n",
    "    print(f\"\\n\u2705 submission.json created successfully!\")\n",
    "    print(f\"\u2705 File size: {os.path.getsize('submission.json')} bytes\")\n",
    "    print(f\"\u2705 Task count: {len(solutions_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT: Visualization \ucf54\ub4dc \uc81c\uac70\ub428\n",
    "\n",
    "**submission.json \uc0dd\uc131\uc744 \ubcf4\uc7a5\ud558\uae30 \uc704\ud574 visualization \ucf54\ub4dc\ub97c \uc81c\uac70\ud588\uc2b5\ub2c8\ub2e4.**\n",
    "\n",
    "\uc774\uc81c:\n",
    "- Run All \u2192 submission.json \uc0dd\uc131 \u2713\n",
    "- Submit to Competition \u2192 \uc815\uc0c1 \uc81c\ucd9c \u2713"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "sourceId": 91496,
     "databundleVersionId": 11802066,
     "sourceType": "competition"
    },
    {
     "sourceId": 12983463,
     "sourceType": "datasetVersion",
     "datasetId": 7970930
    }
   ],
   "dockerImageVersionId": 30919,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}